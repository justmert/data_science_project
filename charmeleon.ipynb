{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# In this project, we are going to do data handling, exploratory data analysis on train.csv\n","# and make house price prediction based on test.csv from the Ames Housing dataset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# importing necessary libraries\n","from sklearn.linear_model import Ridge\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.linear_model import ElasticNet\n","from sklearn.linear_model import Lasso\n","from sklearn import linear_model\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import GridSearchCV\n","from scipy.special import boxcox1p\n","from scipy.stats import skew\n","from sklearn.preprocessing import LabelEncoder\n","from scipy.stats import norm\n","from sklearn.preprocessing import StandardScaler\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set()\n","# %matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Reading the dataset from the csv\n","df_train = pd.read_csv(\"ames_housing/train.csv\")\n","df_test = pd.read_csv(\"ames_housing/test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Quick glance over the train set\n","df_train.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Printing columns\n","print(df_train.columns)\n","print(\"\\nThere are {} columns in the dataset\".format(len(df_train.columns)))\n","# We can see that there a lot of features ( attributes ) in the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's see the non-null counts and dtypes of the columns/attributes\n","df_train.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Describing the train set\n","df_train.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We are going to drop 'Id' columns since it is unnecessary for the prediction process\n","df_train = df_train.drop(\"Id\", axis=1)\n","df_test = df_test.drop(\"Id\", axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Describing the our target variable 'SalePrice'\n","df_train['SalePrice'].describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plotting distribution of the SalePrice\n","sns.distplot(df_train['SalePrice'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# correlation heatmap of the attributes\n","sns.set(font_scale=1.5)\n","corrmat = df_train.corr()\n","fig, ax = plt.subplots(figsize=(20, 20))\n","sns.heatmap(corrmat, vmax=.8, square=True, ax=ax)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sorted correlation of the SalePrice. This way, we can detect which attributes\n","# are linear relationship with SalePrice\n","corrmat.sort_values(['SalePrice'], ascending=False, inplace=True)\n","corrmat['SalePrice']  # the most important features relative to target"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Now we will do bivariate analysis with SalePrice and most correlated attributes with it\n","# GrLivArea and SalePrice Scatter Plot\n","sns.scatterplot(x='GrLivArea', y='SalePrice', data=df_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TotalBsmtSF and GrLivArea Scatter Plot\n","sns.scatterplot(x='TotalBsmtSF', y='SalePrice', data=df_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TotalBsmtSF and GrLivArea Scatter Plot\n","fig, ax = plt.subplots(figsize=(8, 6))\n","sns.boxplot(x='OverallQual', y=\"SalePrice\", data=df_train, ax=ax)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# YearBuilt and GrLivArea Scatter Plot\n","fig, ax = plt.subplots(figsize=(20, 10))\n","sns.boxplot(x='YearBuilt', y=\"SalePrice\", data=df_train, ax=ax)\n","ax.set_xticklabels(ax.get_xticklabels(), rotation=90)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We are creating annotated correlations heatmap between attributes\n","k = 10\n","sns.set(font_scale=1)\n","cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n","cm = np.corrcoef(df_train[cols].values.T)\n","sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={\n","            'size': 10}, yticklabels=cols.values, xticklabels=cols.values)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plotting pairwise relationship among attributes\n","cols = ['SalePrice', 'OverallQual', 'GrLivArea',\n","        'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n","sns.pairplot(df_train[cols], size=2.5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's remember the scatter plot of GrLivArea and SalePrice\n","sns.scatterplot(x='GrLivArea', y='SalePrice', data=df_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# As we can see from GrLivArea with SalePrice with scatter plot that GrLivArea could had outliers.\n","# Outliers are trouble that we should be aware of. Because outliers can be markedly affect our models\n","# and can be a valuable source of information, providing us insights about spesific behaviours.\n","# The two values with bigger ‘GrLivArea’ seem strange and they are not following the crowd.\n","# We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price.\n","# I'm not sure about this but I'm quite confident that these two points are not representative of the typical case.\n","# Therefore, we had defined them as outliers and deleted them\n","\n","df_train = df_train.drop(df_train[(df_train['GrLivArea'] > 4000) & (\n","    df_train['SalePrice'] < 300000)].index)\n","df_train = df_train.drop(df_train[(df_train['OverallQual'] < 5) & (\n","    df_train['SalePrice'] > 200000)].index)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# After deleting outliers, GrLivArea with SalePrice with scatter plot becomes this\n","fig, ax = plt.subplots()\n","sns.scatterplot(x='GrLivArea', y='SalePrice', data=df_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's remember the distribution of SalePrice\n","sns.distplot(df_train['SalePrice'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Printing Skewness and Kurtosis of the SalePrice\n","print('Skewness of SalePrice: {0}'.format(df_train['SalePrice'].skew()))\n","print('Kurtosis of SalePrice: {0}'.format(df_train['SalePrice'].kurt()))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# The SalePrice is skewed to the right. This is a problem because most ML models don't do well with non-normally distributed data.\n","# We can apply a log(1+x) tranform to fix the skew.\n","df_train['SalePrice'] = np.log1p(df_train['SalePrice'])\n","sns.distplot(df_train['SalePrice'], fit=norm, color='b')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Data Processing Stage\n","# We are going to impute missing values and fix the dtypes of attributes\n","# so it is good to concatenating train set and test set into a dataframe and do processing on it.\n","ntrain = df_train.shape[0]\n","ntest = df_test.shape[0]\n","y_train = df_train['SalePrice'].values\n","\n","all_data = pd.concat([df_train, df_test]).reset_index(drop=True)\n","all_data.drop(['SalePrice'], axis=1, inplace=True)\n","all_data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Printing total missing values and percentage of empytness of attributes\n","total = all_data.isnull().sum().sort_values(ascending=False)\n","percent = (all_data.isnull().sum() / all_data.isnull().count()) * 100\n","missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n","missing_data.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plotting missing value percentage of the attributes\n","fig, ax = plt.subplots(figsize=(8, 6))\n","sns.barplot(x=missing_data[:20].index, y='Percent',\n","            data=missing_data[:20], ax=ax)\n","ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n","ax.set(title='Percent of missing values')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# The dataset description page was saying that some of the features’ missing values indicate that there is no such a thing.\n","# For example, If there is a house that has no pool, the dataset will indicate ‘PoolQC’ of the house as empty/missing value.\n","# Altough it is best to delete features if it has empty values more than %85, we avoid to delete in this problem.\n","# ‘PoolQC’ would be an important feature so if we delete it, we could lose some insightful information.\n","# Instead we had inserted ‘None’ value as indicating emptyness to the missing values.\n","# Morover this method was not applicable to the some features.\n","# Those are just missing values that does not indicate empytiness of that thing\n","# so we have filled missing values with mean or median value of the that particular feature\n","\n","# Imputing missing values of attributes\n","\n","all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n","\n","all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n","\n","all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n","\n","all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n","\n","all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n","\n","all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n","    lambda x: x.fillna(x.median()))\n","\n","for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n","    all_data[col] = all_data[col].fillna('None')\n","\n","for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n","    all_data[col] = all_data[col].fillna(0)\n","\n","for col in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n","    all_data[col] = all_data[col].fillna(0)\n","\n","for col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n","    all_data[col] = all_data[col].fillna('None')\n","\n","all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n","\n","all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n","\n","all_data['MSZoning'] = all_data['MSZoning'].fillna(\n","    all_data['MSZoning'].mode()[0])\n","\n","all_data = all_data.drop(['Utilities'], axis=1)\n","\n","all_data['Electrical'] = all_data['Electrical'].fillna(\n","    all_data['Electrical'].mode()[0])\n","\n","all_data['KitchenQual'] = all_data['KitchenQual'].fillna(\n","    all_data['KitchenQual'].mode()[0])\n","\n","all_data['Exterior1st'] = all_data['Exterior1st'].fillna(\n","    all_data['Exterior1st'].mode()[0])\n","\n","all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(\n","    all_data['Exterior2nd'].mode()[0])\n","\n","all_data['SaleType'] = all_data['SaleType'].fillna(\n","    all_data['SaleType'].mode()[0])\n","\n","all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check remaining missing values if any\n","all_data.isnull().values.any()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# There were some numeric types that their dtype was not numeric such as attribute YrSold.\n","# We are converting them to the string types and in the next cell, encoded all categorical features with the LabelEncoder function from sklearn\n","all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n","all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n","all_data['YrSold'] = all_data['YrSold'].astype(str)\n","all_data['MoSold'] = all_data['MoSold'].astype(str)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["categorical_features = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond',\n","                        'ExterQual', 'ExterCond', 'HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1',\n","                        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n","                        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond',\n","                        'YrSold', 'MoSold']\n","\n","# process columns, apply LabelEncoder to categorical features\n","for col in categorical_features:\n","    label = LabelEncoder()\n","    all_data[col] = label.fit_transform(list(all_data[col].values))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We are doing a little bit feature engineering\n","all_data['TotalSF'] = all_data['TotalBsmtSF'] + \\\n","    all_data['1stFlrSF'] + all_data['2ndFlrSF']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["numerical_features = all_data.dtypes[all_data.dtypes != 'object'].index\n","\n","# Checking the skewness of all numerical features\n","skewed_features = all_data[numerical_features].apply(\n","    lambda x: skew(x.dropna())).sort_values(ascending=False)\n","skewness = pd.DataFrame({'Skew': skewed_features})\n","skewness.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plotting box plot of the attributes\n","fig, ax = plt.subplots(figsize=(20, 15))\n","ax.set_xscale(\"log\")\n","ax = sns.boxplot(data=all_data[numerical_features], orient='h')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We are applying Box Cox transform on the skewed numerical attributes\n","skewness = skewness[abs(skewness) > 0.75]\n","print(\"There are {} skewed numerical features to Box Cox transform\".format(\n","    skewness.shape[0]))\n","\n","skewed_features = skewness.index\n","lam = 0.15\n","for feature in skewed_features:\n","    all_data[feature] = boxcox1p(all_data[feature], lam)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Getting dummies\n","all_data = pd.get_dummies(all_data)\n","all_data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We had appended train set and test set before.\n","# Now we are seperating them\n","df_train = all_data[:ntrain]\n","df_test = all_data[ntrain:]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# MODELLING\n","\n","# Just defining some variables to future use\n","score_calc = 'neg_mean_squared_error'\n","cross_val_n = 5"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Defining a function that finds best parameters of the estimator\n","# and returns the best score on the test set of that estimator.\n","def grid_overview(grid):\n","    best_score = np.sqrt(-grid.best_score_)\n","    print(\"Estimator: {}\".format(grid.estimator))\n","    print(\"The best score is {:.4f}\\n\".format(best_score))\n","    print(\"The best parameters are: \")\n","    for key, value in grid.best_params_.items():\n","        print(\"{0}: {1}\".format(key, value))\n","    print(\"\\nThe best estimator is: {}\".format(grid.best_estimator_))\n","\n","    return best_score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Some models requires appropriate parameters and it is exhausting to try all of them manually.\n","# so instead, we have defined some possible parameters for the model\n","# and used GridSearchCV function from sklearn to do exhaustive search over defined parameter values for the model/estimator\n","\n","# Defining Linear Regression estimator and possible parameters\n","linear_regression = LinearRegression()\n","parameters = {'fit_intercept': [True, False],\n","              'normalize': [True, False],\n","              'copy_X': [True, False]}\n","\n","# Using GridSearchCV on the estimator to find best parameters of the estimator\n","grid_linear = GridSearchCV(\n","    linear_regression, parameters, cv=cross_val_n, scoring=score_calc)\n","\n","grid_linear.fit(df_train, y_train)  # fitting\n","linear_score = grid_overview(grid_linear)  # getting best score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Defining Ridge Regression estimator and possible parameters\n","ridge = Ridge()\n","parameters = {'alpha': [0.001, 0.005, 0.01, 0.1, 0.5, 1],\n","              'normalize': [True, False],\n","              'tol': [1e-06, 5e-06, 1e-05, 5e-05]}\n","\n","# Using GridSearchCV on the estimator to find best parameters of the estimator\n","grid_ridge = GridSearchCV(\n","    ridge, parameters, cv=cross_val_n, scoring=score_calc)\n","\n","grid_ridge.fit(df_train, y_train)  # fitting\n","ridge_score = grid_overview(grid_ridge)  # getting best score\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Defining Ridge Regression estimator and possible parameters\n","lasso = Lasso()\n","parameters = {'alpha': [1e-03, 0.01, 0.1, 0.5, 0.8, 1],\n","              'normalize': [True, False],\n","              'tol': [1e-06, 1e-05, 5e-05, 1e-04, 5e-04, 1e-03]}\n","\n","# Using GridSearchCV on the estimator to find best parameters of the estimator\n","grid_lasso = GridSearchCV(\n","    lasso, parameters, cv=cross_val_n, scoring=score_calc)\n","\n","grid_lasso.fit(df_train, y_train)  # fitting\n","lasso_score = grid_overview(grid_lasso)  # getting best score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Defining ElasticNet Regression estimator and possible parameters\n","elasticnet = ElasticNet()\n","parameters = {'alpha': [0.1, 1.0, 10],\n","              'max_iter': [1000000],\n","              'l1_ratio': [0.04, 0.05],\n","              'fit_intercept': [False, True],\n","              'normalize': [True, False],\n","              'tol': [1e-02, 1e-03, 1e-04]}\n","\n","# Using GridSearchCV on the estimator to find best parameters of the estimator\n","grid_elasticnet = GridSearchCV(\n","    elasticnet, parameters, cv=cross_val_n, scoring=score_calc)\n","\n","grid_elasticnet.fit(df_train, y_train)  # fitting\n","elasticnet_score = grid_overview(grid_elasticnet)  # getting best score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Defining Decision Tree Regression estimator and possible parameters\n","tree = DecisionTreeRegressor()\n","parameters = {'max_depth': [7, 8, 9, 10], 'max_features': [11, 12, 13, 14],\n","              'max_leaf_nodes': [None, 12, 15, 18, 20], 'min_samples_split': [20, 25, 30],\n","              'presort': [False, True], 'random_state': [5]}\n","\n","# Using GridSearchCV on the estimator to find best parameters of the estimator\n","grid_tree = GridSearchCV(tree, parameters, cv=cross_val_n, scoring=score_calc)\n","\n","grid_tree.fit(df_train, y_train)  # fitting\n","tree_score = grid_overview(grid_tree)  # getting best score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Defining Random Forest Regression estimator and possible parameters\n","random_forest = RandomForestRegressor()\n","parameters = {'min_samples_split': [3, 4, 6, 10],\n","              'n_estimators': [70, 100],\n","              'random_state': [5]}\n","\n","# Using GridSearchCV on the estimator to find best parameters of the estimator\n","grid_random_forest = GridSearchCV(\n","    random_forest, parameters, cv=cross_val_n, scoring=score_calc)\n","\n","grid_random_forest.fit(df_train, y_train)  # fitting\n","random_forest_score = grid_overview(grid_random_forest)  # getting best score\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Defining KNN Regression estimator and possible parameters\n","knn = KNeighborsRegressor()\n","parameters = {'n_neighbors': [3, 4, 5, 6, 7, 10, 15],\n","              'weights': ['uniform', 'distance'],\n","              'algorithm': ['ball_tree', 'kd_tree', 'brute']}\n","\n","# Using GridSearchCV on the estimator to find best parameters of the estimator\n","grid_knn = GridSearchCV(knn, parameters, cv=cross_val_n,\n","                        scoring=score_calc, refit=True)\n","\n","grid_knn.fit(df_train, y_train)  # fitting\n","knn_score = grid_overview(grid_knn)  # getting best score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Gathering all scores in a list\n","all_scores = [linear_score, ridge_score, lasso_score, elasticnet_score,\n","              tree_score, random_forest_score, knn_score]\n","\n","all_regressors = ['Linear', 'Ridge', 'Lasso', 'ElaNet', 'DTR', 'RF', 'KNN']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plotting RMSE Score of all estimators\n","fig, ax = plt.subplots(figsize=(8, 6))\n","sns.barplot(x=all_scores, y=all_regressors, ax=ax).set(\n","    title='RMSE Score of all Estimators', xlabel='RMSE Score', ylabel='Estimators/Models')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Making prediction based on defined models\n","pred_linear = grid_linear.predict(df_test)\n","pred_ridge = grid_ridge.predict(df_test)\n","pred_lasso = grid_lasso.predict(df_test)\n","pred_elanet = grid_elasticnet.predict(df_test)\n","pred_dtr = grid_tree.predict(df_test)\n","pred_rf = grid_random_forest.predict(df_test)\n","pred_knn = grid_knn.predict(df_test)\n","\n","predictions = {'Linear': pred_linear,\n","               'Ridge': pred_ridge,\n","               'Lasso': pred_lasso,\n","               'ElaNet': pred_elanet,\n","               'DTR': pred_dtr,\n","               'RandomF': pred_rf,\n","               'KNN': pred_knn}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# calculating correlations between predictions\n","df_predictions = pd.DataFrame(data=predictions)\n","df_predictions.corr()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plotting correlations heatmap between predictions\n","pred_corr = df_predictions.corr()\n","fig, ax = plt.subplots(figsize=(7, 7))\n","sns.set(font_scale=1.25)\n","sns.heatmap(pred_corr, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={\n","            'size': 10}, yticklabels=df_predictions.columns, xticklabels=df_predictions.columns)"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}